---
title: "Paper/Concept Title (Year)"
aliases: 
  - Acronym
  - Alternate Title
authors:
  - Author 1
  - Author 2
year: YYYY
venue: "Conference / Journal"
doi: "10.48550/arXiv.xxxxx"
arxiv: "https://arxiv.org/abs/xxxx.xxxxx"
code: "https://github.com/.../repo"
citations: 0
dataset:
  - Dataset1
  - Dataset2
tags:
  - paper
  - deep-learning
  - computer-vision
fields:
  - vision
  - nlp
  - generative-models
related:
  - "[[Related Note 1]]"
  - "[[Related Note 2]]"
predecessors:
  - "[[Predecessor Paper 1]]"
  - "[[Predecessor Paper 2]]"
successors:
  - "[[Successor Paper 1]]"
  - "[[Successor Paper 2]]"
impact: ⭐⭐☆☆☆   # subjective importance (1–5 stars)
status: "to-read" # options: to-read / read / implemented
---

# Summary
Write a short description of the paper or concept. Include what it solves and why it is important.

# Key Idea
> One-liner tagline of the main contribution.

# Method
- Main components or architecture.  
- Key equations or algorithm steps.  
- Training strategy / supervision used.  

# Results
- Benchmarks and datasets.  
- Improvements vs baselines.  
- Notable numbers (accuracy, FID, etc.).  

# Why it Mattered
- Historical significance.  
- Shift in paradigm / impact on later research.  

# Architectural Pattern
- Core building blocks used (e.g., CNN backbone, Transformer, Autoencoder).  
- Patterns repeated in later works.  

# Connections
- **Contemporaries**: Similar time-frame papers.  
- **Influence**: Broader impact on research / industry.  

# Implementation Notes
- Training details, tricks, approximations.  
- Framework/library considerations.  
- Any practical gotchas.  

# Critiques / Limitations
- Weaknesses of the method.  
- Where it underperforms.  
- Superseded by later work?  

# Repro / Resources
- Paper link.  
- Official code repo.  
- Datasets used.  
- Good tutorials/implementations.  

---

# Educational Connections

## Undergraduate-Level Concepts
- **Linear Algebra**: e.g., convolutions, embeddings, projections.  
- **Probability & Statistics**: e.g., loss functions, distributions.  
- **Calculus**: e.g., backpropagation, gradients.  
- **Signals & Systems**: e.g., filtering, sampling.  
- **Data Structures**: e.g., tensors, embeddings.  
- **Optimization Basics**: e.g., SGD, Adam.  

## Postgraduate-Level Concepts
- **Advanced Optimization**: e.g., curriculum learning, EMA, large-batch training.  
- **Numerical Methods**: e.g., sampling methods, approximations.  
- **Machine Learning Theory**: e.g., inductive bias, generalization.  
- **Computer Vision / NLP**: specific task-level contributions.  
- **Neural Network Design**: architecture innovations.  
- **Transfer Learning**: zero-shot, fine-tuning, domain adaptation.  
- **Research Methodology**: benchmarks, ablations, scaling laws.  

---

# My Notes
- How this connects to my projects:  
- Open questions:  
- Possible extensions:  
