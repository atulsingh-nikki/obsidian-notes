---
title: "Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling"
aliases:
  - Foraging with the Eyes
  - Gaze as LÃ©vy Walk
authors:
  - Tejaswi V. Panchagnula
affiliation: "Indian Institute of Technology Madras"
year: 2025
venue: "arXiv preprint arXiv:2510.09299"
citations: "New, July 2025"
tags:
  - paper
  - eye-tracking
  - gaze-modeling
  - human-vision
  - levy-walk
  - cnn
fields:
  - cognitive-vision
  - human-computer-interaction
  - computational-neuroscience
impact: â­â­â­â­
status: "read"
related:
  - "[[Learning to Predict Where Humans Look (Judd et al., 2009)]]"
  - "[[DeepGaze I (KÃ¼mmerer et al., 2015)]]"
  - "[[Beyond Pixels: Predicting Human Gaze (Xu et al., 2014)]]"

---

# ðŸ§  Summary

This study proposes that **human visual gaze behaves like a LÃ©vy walk**, similar to **animal foraging** in sparse environments.  
It bridges **visual attention modeling** with **movement ecology**, showing that human eyes â€œforageâ€ for information optimally.

> The gaze trajectory is not random noise â€” it follows heavy-tailed step length distributions, balancing short fixations with long exploratory saccades.

The author further builds a **CNN-based fixation prediction model** that learns attention heatmaps from images, demonstrating the *learnability of spatial fixation behavior* â€” though not its full temporal stochasticity.

---

# ðŸ§© Key Insights

| Aspect | Description |
|--------|--------------|
| **Core Hypothesis** | Human gaze follows a LÃ©vy walk â€” a heavy-tailed stochastic process optimized for information foraging. |
| **Experiment** | 40 participants, 50 diverse images, 4 million gaze points via 120 Hz Aurora Smart Eye Tracker. |
| **Key Finding** | Eye movements exhibit power-law distributed step lengths (1 < Î¼ â‰¤ 3), indicating LÃ©vy-like exploration. |
| **Entropy Correlation** | Higher image entropy â†’ longer average gaze jumps. |
| **Turning Angle** | Bimodal distribution â€” vertical preference (Â±90Â°) and straight-line persistence (0 radians). |
| **CNN Model** | Predicts fixation heatmaps from images using MobileNetV2 + U-Net decoder. |
| **Loss** | BCE + MSE + KL divergence composite loss. |
| **Outcome** | Accurately predicts fixation clusters but fails to model long-range LÃ©vy-like jumps. |

---

# ðŸ”¬ Methodology

### ðŸ§ª Data Collection
- 40 participants, free-viewing 50 images for 30 s each.
- Two groups (25 images each) balanced by image entropy.
- ~110k gaze points per subject; ~4 million total.
- Entropy computed via Shannon measure \(H = -\sum p(i)\log_2 p(i)\).

### ðŸ“Š Statistical Analysis
- Step length = distance between successive gaze points.
- Logâ€“log slope of âˆ’2.2 â†’ âˆ’2.4 per image â‡’ LÃ©vy regime.
- Cumulative slope â‰ˆ âˆ’3.5 â†’ Gaussian aggregation (Central Limit Theorem).
- Positive correlation between image entropy and LÃ©vy slope (Î¼).
- Turning angles show structured exploration (not random).

### ðŸ§  Modeling
- **Architecture:** MobileNetV2 encoder + transposed conv decoder â†’ fixation map.
- **Loss:** \(L = 0.4Â·BCE + 0.3Â·MSE + 0.3Â·DKL(Hâ€–\hat{H})\)
- **Training:** AdamW, cosine annealing, 10 epochs.
- **Output:** \( \hat{H} = D(E(I)) \), where \(\hat{H}\) = predicted fixation heatmap.

### ðŸ“ˆ Results
- High-quality fixation maps with strong spatial correlation.
- Validation â‰ˆ training loss (good generalization).
- Captures multimodal saliency clusters, not LÃ©vy-style long saccades.

---

# ðŸŒ Conceptual Link â€” â€œForaging with the Eyesâ€

- Eye movement = **information foraging**.
- Short fixations â†” local inspection; long saccades â†” exploratory search.
- Analogous to **albatross flight patterns** and **animal foraging** under sparse-resource conditions.
- Visual system exhibits *optimal efficiency within biological energy constraints (~15 W)*.

---

# ðŸ§© Key Equations

1. **Entropy of an Image:**
   \[
   H = -\sum_{i=0}^{L-1} p(i)\log_2 p(i)
   \]

2. **LÃ©vy Walk Step Distribution:**
   \[
   P(l) \sim |l|^{-\mu}, \quad 1 < \mu \le 3
   \]

3. **CNN Loss Function:**
   \[
   L = \alpha \, BCE + \beta \, MSE + \gamma \, D_{KL}(H \parallel \hat{H})
   \]

---

# âš™ï¸ Model Evaluation

| Metric | Observation |
|--------|--------------|
| **Qualitative Fit** | Accurate clustering of salient zones |
| **Quantitative** | BCE, MSE, KL losses converge smoothly |
| **Failure Mode** | Misses heavy-tailed long jumps |
| **Interpretation** | Static heatmaps â‰  dynamic gaze trajectories |

---

# ðŸ§­ Discussion Highlights

- Gaze is **stochastic but structured**, shaped by both image entropy and human cognitive patterns.
- **Static saliency models** ignore the sequential nature of attention.
- **Autoregressive RNN/Transformer models** failed â€” gaze lacks consistent temporal order.
- Predictive modeling benefits from *distributional rather than sequential* representations.
- Applications:
  - Attention prediction for AR/VR
  - Adaptive HCI interfaces
  - Early diagnosis (autism, ADHD, neurodegeneration)

---

# ðŸ“š Connections

| Domain | Insight |
|--------|----------|
| **Movement Ecology** | Visual scanpaths = LÃ©vy-like foraging trajectories |
| **Cognitive Science** | Eye movements reveal information optimization |
| **Computer Vision** | CNNs can learn fixation probability but not dynamic stochasticity |
| **Statistical Physics** | Heavy-tailed distributions in human movement patterns |

---

# ðŸ§© Future Work

- Temporal modeling (e.g., RNN + transformer extensions).
- Gaze-conditioned generative models.
- Personalized gaze modeling across demographics.
- Coupling entropy-based image priors with dynamic foraging theory.

---

# ðŸ“– References
Key works cited include:
- Judd et al. *Learning to Predict Where Humans Look* (ICCV 2009)
- Xu et al. *Predicting Human Gaze Beyond Pixels* (JoV 2014)
- KÃ¼mmerer et al. *DeepGaze I* (ICLR 2015)
- Viswanathan et al. *LÃ©vy Flights in Random Searches* (Physica A 2000)
- Martinez-Conde et al. *Microsaccades and Visual Stability* (Nat Rev Neuro 2013)
- Brockmann et al. *Scaling Laws of Human Travel* (Nature 2006):contentReference[oaicite:1]{index=1}

---

# ðŸ§­ Takeaway

> The human visual system "forages" for information with LÃ©vy-like efficiency â€”  
> blending local fixation with global exploration â€”  
> a bridge between **ecological movement**, **statistical physics**, and **cognitive vision**.

---
