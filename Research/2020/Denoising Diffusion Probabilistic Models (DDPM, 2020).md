---
title: "Denoising Diffusion Probabilistic Models (DDPM, 2020)"
aliases:
  - DDPM
  - Denoising Diffusion Probabilistic Models
authors:
  - Jonathan Ho
  - Ajay Jain
  - Pieter Abbeel
year: 2020
venue: "NeurIPS"
doi: "10.48550/arXiv.2006.11239"
arxiv: "https://arxiv.org/abs/2006.11239"
code: "https://github.com/hojonathanho/diffusion"
citations: 20,000+
dataset:
  - CIFAR-10
  - LSUN
  - CelebA-HQ
  - ImageNet subsets
tags:
  - paper
  - diffusion-model
  - generative-model
  - denoising
  - probabilistic
fields:
  - vision
  - generative-models
  - deep-learning
related:
  - "[[Score-Based Generative Modeling (2019–2020)]]"
  - "[[DDIM (2020)]]"
  - "[[Latent Diffusion Models (2022)]]"
predecessors:
  - "[[Score Matching (Hyvärinen, 2005)]]"
successors:
  - "[[DDIM (2020)]]"
  - "[[Latent Diffusion Models (2022)]]"
impact: ⭐⭐⭐⭐⭐
status: "read"

---

# Summary
**DDPM** introduced modern **diffusion probabilistic models** as a powerful class of generative models. It showed that images could be generated by **reversing a diffusion process**: start from Gaussian noise, iteratively denoise with a neural network, and reconstruct a clean image. This paper revived diffusion models, showing they could match or beat GANs in sample quality.

# Key Idea
> Train a neural network to predict and remove noise added in a forward **diffusion process**, then reverse this process at inference to generate images from pure noise.

# Method
- **Forward process**: Add Gaussian noise to data step by step until it becomes pure noise.  
- **Reverse process**: Train a U-Net to estimate the added noise at each step.  
- **Loss**: Simplified to an MSE loss between predicted and true noise.  
- **Generation**: Start from Gaussian noise, apply learned denoising steps iteratively (e.g., 1000 steps).  

# Results
- Achieved high-quality image synthesis on CIFAR-10, LSUN, and CelebA-HQ.  
- Comparable to or better than GANs in FID.  
- Showed robustness (no mode collapse, stable training).  

# Why it Mattered
- Revived diffusion models after being largely ignored since the 2010s.  
- Provided a **simple, stable alternative to GANs** for generative modeling.  
- Inspired a flood of follow-up work (DDIM, score-SDE, LDM, Stable Diffusion).  

# Architectural Pattern
- U-Net backbone with skip connections.  
- Time-step embedding for conditioning.  
- Probabilistic denoising formulation.  

# Connections
- Parallel to **Score-Based Generative Modeling (Song & Ermon, 2019–2020)**.  
- Direct predecessor to **DDIM (2020)** (fast sampling).  
- Inspired **Latent Diffusion Models (2022)**, the foundation of Stable Diffusion.  

# Implementation Notes
- Requires many sampling steps (slow at inference).  
- Training stable and simple compared to GANs.  
- Conditioning (e.g., class labels) easily integrated.  

# Critiques / Limitations
- Sampling very slow (hundreds–thousands of steps).  
- Memory heavy for high resolutions.  
- Less interpretable than GAN latent spaces (until DDIM inversion).  

---

# Educational Connections

## Undergraduate-Level Concepts
- Idea of adding/removing noise.  
- Markov chain processes.  
- Denoising autoencoder intuition.  

## Postgraduate-Level Concepts
- Probabilistic formulation of diffusion.  
- Variational lower bound derivation.  
- Connection to score matching & SDEs.  

---

# My Notes
- DDPM was the **big bang** of the diffusion wave.  
- GANs were king until this showed a more stable, elegant way to generate.  
- Open question: How to make DDPMs **efficient** (solved by DDIM, consistency models).  
- Possible extension: Use DDPM backbone for **video diffusion with temporal conditioning**.  

---
