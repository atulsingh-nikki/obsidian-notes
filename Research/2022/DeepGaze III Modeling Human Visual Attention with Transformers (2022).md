---
title: "DeepGaze III: Modeling Human Visual Attention with Transformers (2022)"
aliases:
  - DeepGaze 3
  - DeepGaze III
authors:
  - Matthias KÃ¼mmerer
  - Thomas S. A. Wallis
  - Matthias Bethge
year: 2022
venue: Neural Information Processing Systems (NeurIPS)
tags:
  - paper
  - deep-learning
  - transformers
  - saliency
  - human-vision
  - probabilistic-modeling
fields:
  - computer-vision
  - computational-neuroscience
  - cognitive-science
impact: â­â­â­â­â­â­
status: read
related:
  - "[[Learning to Predict Where Humans Look (ICCV 2009)]]"
  - "[[Foraging with the Eyes Dynamics in Human Visual Gaze and Deep Predictive Modeling|Foraging with the Eyes]]"
  - "[[DeepGaze II Predicting Fixations with Deeper Features and Probabilistic Modeling (2017)|DeepGaze 2]]"
---

# ðŸ§  Summary
**DeepGaze III** extended the DeepGaze lineage into the **transformer era**, incorporating **Vision Transformers (ViT)** for feature extraction and a **probabilistic attention decoder** for fixation prediction.  
It achieved **human-level performance** on the MIT300 and CAT2000 benchmarks â€” the first model to do so.

> â€œFor the first time, we can quantitatively match human gaze prediction â€” not by mimicking heuristics, but by modeling the underlying perceptual representation.â€

---

# ðŸ§© Key Insights

| Aspect | Description |
|--------|--------------|
| **Feature Backbone** | Vision Transformer (ViT-B/16 pretrained on ImageNet-21K). |
| **Decoder** | Lightweight CNN + learned spatial prior for fixation density. |
| **Learning Objective** | Probabilistic log-likelihood + information gain (as in DeepGaze II). |
| **Result** | Achieved human inter-observer-level agreement on major benchmarks. |
| **Significance** | Unified transformer-level perception with probabilistic neuroscience modeling. |

---

# ðŸ”¬ Methodology

### 1ï¸âƒ£ Feature Extraction
- Extract multi-scale features from **ViT** encoder layers (tokens reshaped to spatial grids).
- Each token encodes global and contextual information, improving scene-level reasoning.

### 2ï¸âƒ£ Fixation Decoder
- Multi-layer convolutional projection maps transformer embeddings â†’ saliency scores.
- Additive **learned center bias** (as in DeepGaze II):
  \[
  P(x,y|I) = \frac{e^{S(x,y) + C(x,y)}}{\sum_{x',y'} e^{S(x',y') + C(x',y')}}
  \]

### 3ï¸âƒ£ Training
- Optimize **log-likelihood of fixation points**:
  \[
  \mathcal{L} = -\sum_{(x,y)} \log P(x,y|I)
  \]
- Information-theoretic metrics (IG) for model comparison.

### 4ï¸âƒ£ Evaluation
- Datasets: MIT1003, MIT300, CAT2000, COCO-Freeview.
- Metrics: Information Gain (IG), NSS, KL divergence.

---

# ðŸ“Š Results

| Benchmark | IG (bits/fixation) | Status |
|------------|--------------------|---------|
| **MIT300** | 1.19 Â± 0.02 | Matches human IOB baseline |
| **CAT2000** | 1.15 | State of the art |
| **COCO-Freeview** | 1.11 | Best reported |
| **AUC / NSS** | Comparable to or exceeding human fixations |

---

# ðŸŒ Impact and Legacy
- **First model** to achieve **human-level attention prediction** under benchmark evaluation.  
- Reframed saliency as a **representation-learning** problem rather than explicit feature engineering.  
- Demonstrated that **transformer attention** can mirror **biological visual attention**.  
- Serves as the foundation for **neural saliency maps in multimodal models** (e.g., CLIP, Flamingo, GPT-4V attention studies).

---

# âš™ï¸ Architecture Overview

---
Image â†’ Vision Transformer (ViT) â†’ spatial feature tokens â†’ CNN decoder â†’ saliency map S(x,y)  
+ learned center prior â†’ softmax â†’ fixation probability P(x,y|I)
---

# ðŸ’¬ Discussion Highlights
- ViT attention heads mirror biological foveation and scene parsing.  
- Learned global context explains why DeepGaze III captures **object-level saliency** without explicit face/text priors.  
- Probabilistic framing continues to outperform purely pixel-wise regression.  
- Empirical finding: fixation entropy correlates with semantic richness of the image.  

---

# ðŸ§© Educational Connections

| Level | Concepts |
|--------|----------|
| **Undergraduate** | Vision Transformers, attention maps, saliency prediction. |
| **Graduate** | Probabilistic modeling of gaze, information gain metrics, humanâ€“AI perceptual alignment. |

---

# ðŸ“š References
- Judd et al., *Learning to Predict Where Humans Look*, ICCV 2009  
- KÃ¼mmerer et al., *DeepGaze II*, CVPR 2017  
- Dosovitskiy et al., *ViT*, ICLR 2021  
- KÃ¼mmerer et al., *DeepGaze III*, NeurIPS 2022  

---

# ðŸ§­ Takeaway
> **DeepGaze III** closes the humanâ€“machine gap in saliency prediction â€”  
> using transformer features and probabilistic modeling to match the way humans allocate visual attention.
